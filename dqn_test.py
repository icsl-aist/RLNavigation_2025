#!/usr/bin/env python3
#################################################################################
# PPO (Continuous Action) Test Node
# - Loads PPO Actor model (PyTorch)
# - Performs inference to get continuous action [linear_v, angular_v]
# - Communicates with RLEnvironment via 'rl_agent_interface' service
# - ğŸ¯ 100å›å®Ÿè¡Œã—ã¦æˆåŠŸç‡ã‚’ç®—å‡ºã™ã‚‹ã‚ˆã†å¤‰æ›´
#################################################################################

import os
import sys
import time
import numpy as np

import rclpy
from rclpy.node import Node
import torch
import torch.nn as nn
import torch.distributions

from turtlebot3_msgs.srv import Dqn as DqnSrv # Dqnã‚µãƒ¼ãƒ“ã‚¹ã‚’PPOã®I/Fã¨ã—ã¦åˆ©ç”¨


# ============================================================
# PPO Actor ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ï¼ˆdqn_agent.py ã‹ã‚‰ç§»æ¤ï¼‰
# ============================================================
class Actor(nn.Module):
    """PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Actorãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆæ¨è«–å°‚ç”¨ï¼‰"""
    def __init__(self, state_dim=28, action_dim=2):
        super().__init__()
        # state_dim=28 ã¯ã€dqn_agent.py ã® state_size (26) + prev_action (2)
        self.net = nn.Sequential(
            nn.Linear(state_dim, 256), nn.ReLU(),
            nn.Linear(256, 128), nn.ReLU(),
        )
        self.mu_layer = nn.Linear(128, action_dim)
        # log_std ã¯å­¦ç¿’æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
        self.log_std = nn.Parameter(torch.zeros(action_dim))

    def forward(self, x):
        x = self.net(x)
        mu = torch.tanh(self.mu_layer(x))
        std = torch.exp(self.log_std).clamp(1e-3, 1.0)
        return mu, std # mu:å¹³å‡ï¼ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ä¸­å¿ƒï¼‰, std:æ¨™æº–åå·®


# ============================================================
# PPO ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒ‰
# ============================================================
class PPOTest(Node):
    """TurtleBot3 PPOãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒãƒ¼ãƒ‰"""

    def __init__(self, stage, load_episode, num_tests=50):
        super().__init__('ppo_test')

        self.stage = int(stage)
        self.load_episode = int(load_episode)
        self.num_tests = int(num_tests) # ğŸ‘ˆ å®Ÿè¡Œå›æ•° 100å›

        self.state_size = 26      # LiDAR + Goal Info
        self.action_size = 2      # [linear, angular]
        self.state_input_size = self.state_size + self.action_size # 28 (State + Prev Action)

        # ---- PPOãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆdqn_agent.py ã‹ã‚‰ã®ç§»æ¤ï¼‰----
        self.lin_low, self.lin_high = -0.05, 0.30
        self.ang_low, self.ang_high = -1.5, 1.5
        self.device = torch.device('cpu')

        # ---- ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã¨ãƒ­ãƒ¼ãƒ‰ ----
        self.actor = Actor(self.state_input_size, self.action_size).to(self.device)
        self.load_model()

        # ---- ROSã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ ----
        self.rl_agent_interface_client = self.create_client(DqnSrv, 'rl_agent_interface')
        self.reset_env_client = self.create_client(DqnSrv, 'reset_environment')


        # ---- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹ ----
        self.run_test()

    def load_model(self):
        """å­¦ç¿’æ¸ˆã¿Actorãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ PPO ã®å‘½åè¦å‰‡ã«åˆã‚ã›ã‚‹
        model_path = os.path.join(
            os.path.dirname(os.path.dirname(os.path.realpath(__file__))),
            'saved_model',
            f'stage{self.stage}_ep{self.load_episode}_actor.pth'
        )

        try:
            self.actor.load_state_dict(torch.load(model_path, map_location=self.device))
            self.actor.eval() # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.get_logger().info(f'ğŸ¤– Actorãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {model_path}')
        except Exception as e:
            self.get_logger().error(f'âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ ({model_path}): {e}')
            sys.exit(1) # ãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚ã¯çµ‚äº†

    def _scale_actions(self, a_norm2: torch.Tensor) -> tuple:
        """
        [-1, 1]^2 ã®æ­£è¦åŒ–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿãƒ¬ãƒ³ã‚¸ã«ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆdqn_agent.pyã‹ã‚‰ç§»æ¤ï¼‰
        """
        lin = (a_norm2[0].clamp(-1, 1).item() + 1.0) * 0.5 * (self.lin_high - self.lin_low) + self.lin_low
        ang = (a_norm2[1].clamp(-1, 1).item() + 1.0) * 0.5 * (self.ang_high - self.ang_low) + self.ang_low
        return (lin, ang)

    def get_action(self, state_plus_prev):
        """
        PPO Actorãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠï¼ˆæ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼‰
        """
        s = torch.from_numpy(state_plus_prev).to(device=self.device, dtype=torch.float32).unsqueeze(0) # (1, 28)

        with torch.no_grad():
            # æ¨è«–æ™‚ã¯æ¨™æº–åå·®ã‚’ä½¿ã‚ãšã€æ±ºå®šè«–çš„ã«å¹³å‡å€¤ï¼ˆÎ¼ï¼‰ã‚’ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ã™ã‚‹
            mu, _ = self.actor(s)               # (1,2), (1,2)
            a_norm = torch.tanh(mu).squeeze(0)  # [-1,1] ã«åœ§ç¸® (2,)

        return self._scale_actions(a_norm) # å®Ÿãƒ¬ãƒ³ã‚¸ [lin, ang] ã«å¤‰æ›

    def reset_environment(self):
        """ç’°å¢ƒãƒªã‚»ãƒƒãƒˆã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã³å‡ºã—ã€åˆæœŸçŠ¶æ…‹ã‚’å–å¾—"""
        while not self.reset_env_client.wait_for_service(timeout_sec=1.0):
            self.get_logger().warn('reset_environment ã‚µãƒ¼ãƒ“ã‚¹å¾…æ©Ÿä¸­...')
        future = self.reset_env_client.call_async(DqnSrv.Request())
        rclpy.spin_until_future_complete(self, future)
        
        if future.result() is None:
            self.get_logger().error('ç’°å¢ƒãƒªã‚»ãƒƒãƒˆå¤±æ•—: ã‚¼ãƒ­çŠ¶æ…‹ã§ç¶™ç¶š')
            state = np.zeros(self.state_size, np.float32)
        else:
            state = np.asarray(future.result().state, np.float32) # (26,)

        return state


    def run_test(self):
        self.get_logger().info(f'--- PPO Test START: {self.num_tests} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ ---')
        
        episode_count = 0
        success_count = 0 # ğŸ‘ˆ æˆåŠŸå›æ•°ã‚’è¨˜éŒ²

        # ğŸ‘ˆ å®Ÿè¡Œå›æ•°ã®ä¸Šé™ã‚’è¨­å®š
        while rclpy.ok() and episode_count < self.num_tests:
            episode_count += 1
            
            # --- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹ ---
            state = self.reset_environment() # (26,)
            done = False
            succeed = False # ğŸ‘ˆ æˆåŠŸãƒ•ãƒ©ã‚°
            score = 0.0
            local_step = 0
            
            # PPOã§ã¯ã€æœ€åˆã®è¡Œå‹•ã‚’å‡ºã™å‰ã«ã€Œå‰å›ã®è¡Œå‹•ã€prev_action = [0.0, 0.0] ã‚’çµåˆã™ã‚‹
            prev_action = np.zeros(self.action_size, np.float32) # (2,)

            self.get_logger().info(f'\n--- EPISODE {episode_count}/{self.num_tests} START ---')
            time.sleep(1.0)

            while not done:
                local_step += 1
                
                # 1. çŠ¶æ…‹ã«å‰å›ã®è¡Œå‹•ã‚’çµåˆ (26+2=28)
                state_plus_prev = np.concatenate([state, prev_action], axis=0)

                # 2. Actorã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ [linear_v, angular_v] ã‚’å–å¾—
                lin_v, ang_v = self.get_action(state_plus_prev)
                
                # 3. ç’°å¢ƒã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é€ä¿¡
                req = DqnSrv.Request()
                req.action = [float(lin_v), float(ang_v)]

                while not self.rl_agent_interface_client.wait_for_service(timeout_sec=1.0):
                    self.get_logger().warn('rl_agent interface service not available, waiting again...')

                future = self.rl_agent_interface_client.call_async(req)
                rclpy.spin_until_future_complete(self, future)

                if future.done() and future.result() is not None:
                    res = future.result()
                    state = np.asarray(res.state, np.float32) # æ¬¡ã®çŠ¶æ…‹ (26,)
                    reward = res.reward
                    done = res.done
                    succeed = res.success # ğŸ‘ˆ æˆåŠŸãƒ•ãƒ©ã‚°ã‚’å–å¾—
                    score += reward
                    prev_action = np.array([lin_v, ang_v], np.float32) # ä»Šå›ã®è¡Œå‹•ã‚’æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã® 'prev_action' ã«
                    
                    if local_step % 50 == 0 or done:
                        self.get_logger().info(
                            f"[Step {local_step:04d}] Lin={lin_v:.3f}, Ang={ang_v:.3f} | "
                            f"R={reward:+.3f} | Total R={score:+.3f} | Done={done} (Success: {succeed})"
                        )
                else:
                    self.get_logger().error(f'Service call failure: {future.exception()}')
                    done = True # å¤±æ•—æ™‚ã¯çµ‚äº†
                
                time.sleep(0.01)

            # ğŸ‘ˆ æˆåŠŸå›æ•°ã‚’æ›´æ–°
            if succeed:
                success_count += 1
                result_str = "âœ… SUCCESS"
            else:
                result_str = "âŒ FAILED (Collision or Timeout)"
            
            self.get_logger().info(
                f'ğŸ EPISODE {episode_count}/{self.num_tests} FINISHED in {local_step} steps. {result_str} | Total Score: {score:+.3f}'
            )

            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–“ã«å°‘ã—å¾…æ©Ÿ
            time.sleep(2.0)

        # ----------------------------------------------------
        # æœ€çµ‚çµæœã®å‡ºåŠ›
        # ----------------------------------------------------
        success_rate = (success_count / self.num_tests) * 100.0
        
        self.get_logger().info("=" * 50)
        self.get_logger().info("âœ¨ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼šæˆåŠŸç‡ã®ç®—å‡ºçµæœ")
        self.get_logger().info(f"ç·è©¦è¡Œå›æ•°: {self.num_tests} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        self.get_logger().info(f"æˆåŠŸå›æ•°: {success_count} å›")
        self.get_logger().info(f"æˆåŠŸç‡: {success_rate:.2f}%")
        self.get_logger().info("=" * 50)


def main(args=None):
    if args is None:
        args = sys.argv
        
    # å®Ÿè¡Œå¼•æ•°ã‹ã‚‰ Stage ã¨ Load Episode ã‚’å–å¾—
    stage = args[1] if len(args) > 1 else '1'
    load_episode = args[2] if len(args) > 2 else '600'
    num_tests = args[3] if len(args) > 3 else '100' # ğŸ‘ˆ æ–°ã—ã„å¼•æ•°

    rclpy.init(args=args)
    # ğŸ‘ˆ PPOTest ã®åˆæœŸåŒ–æ™‚ã« num_tests ã‚’æ¸¡ã™
    node = PPOTest(stage, load_episode, num_tests)

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()