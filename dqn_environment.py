#!/usr/bin/env python3
#################################################################################
# PPO (Continuous Action) version of TurtleBot3 RL Environment
# - Compatible with ROS2 Humble
# - Supports continuous angular velocity control (float)
# - Keeps service/topic interfaces for backward compatibility
#################################################################################

import math
import time
import os
import numpy
import rclpy
from rclpy.node import Node
import numpy as np
from rclpy.qos import qos_profile_sensor_data, QoSProfile
from rclpy.callback_groups import MutuallyExclusiveCallbackGroup

from geometry_msgs.msg import Twist, TwistStamped
from nav_msgs.msg import Odometry
from sensor_msgs.msg import LaserScan
from std_srvs.srv import Empty
from turtlebot3_msgs.srv import Dqn, Goal


ROS_DISTRO = os.environ.get('ROS_DISTRO')  # å®Ÿè¡Œç’°å¢ƒãŒ "humble" ã‹åˆ¤å®šç”¨


class RLEnvironment(Node):
    """TurtleBot3ç”¨ å¼·åŒ–å­¦ç¿’ç’°å¢ƒãƒãƒ¼ãƒ‰ï¼ˆPPOé€£ç¶šã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œç‰ˆï¼‰"""

    def __init__(self):
        # ROS 2ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
        super().__init__('rl_environment')

        # ---- ã‚´ãƒ¼ãƒ«ãŠã‚ˆã³ãƒ­ãƒœãƒƒãƒˆä½ç½®çŠ¶æ…‹ ----
        self.goal_pose_x = 0.0
        self.goal_pose_y = 0.0
        self.robot_pose_x = 0.0
        self.robot_pose_y = 0.0
        self.goal_angle = 0.0             # ãƒ­ãƒœãƒƒãƒˆã‹ã‚‰è¦‹ãŸã‚´ãƒ¼ãƒ«æ–¹å‘è§’
        self.goal_distance = 1.0          # ç¾åœ¨ã®ã‚´ãƒ¼ãƒ«è·é›¢
        self.init_goal_distance = 0.5     # åˆæœŸè·é›¢ï¼ˆãƒªã‚»ãƒƒãƒˆæ™‚ï¼‰

        # ---- ã‚¹ãƒ†ãƒƒãƒ—ç®¡ç†ï¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ¶å¾¡ ----
        self.max_step = 1500               # 1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚ãŸã‚Šæœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
        self.local_step = 0               # ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
        self.done = False                 # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãƒ•ãƒ©ã‚°
        self.fail = False                 # å¤±æ•—ãƒ•ãƒ©ã‚°ï¼ˆè¡çªãªã©ï¼‰
        self.succeed = False              # æˆåŠŸãƒ•ãƒ©ã‚°ï¼ˆã‚´ãƒ¼ãƒ«åˆ°é”ï¼‰
        self.stop_cmd_vel_timer = None    # è‡ªå‹•åœæ­¢ç”¨ã‚¿ã‚¤ãƒãƒ¼

        self._ep_steps = 0
        self.episode_count = 0            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ãƒˆã‚’è¿½åŠ 
        self.trajectory_log = []          # è»Œè·¡ãƒ­ã‚°ãƒªã‚¹ãƒˆã‚’è¿½åŠ 

        # ---- å ±é…¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä¿æŒç”¨ (â˜… è¿½åŠ : get_reward_componentsã§å¿…è¦) ----
        self.distance_reward = 0.0
        self.yaw_reward = 0.0
        self.obstacle_reward = 0.0
        self.terminal_reward = 0.0

        # ---- LiDARï¼ˆLaserScanï¼‰æƒ…å ± ----
        self.scan_ranges = []             # å…¨æ–¹å‘è·é›¢
        self.front_ranges = []            # å‰æ–¹è·é›¢ï¼ˆ0ã€œ90Â°, 270ã€œ360Â°ï¼‰
        self.front_angles = []            # å‰æ–¹è§’åº¦ç¾¤
        self.min_obstacle_distance = 10.0 # æœ€è¿‘éšœå®³ç‰©è·é›¢


        # ---- QoS / Publisherè¨­å®š ----
        qos = QoSProfile(depth=10)
        # ROS2ãƒ‡ã‚£ã‚¹ãƒˆãƒ­ã«ã‚ˆã‚ŠTwist/TwistStampedã‚’åˆ‡ã‚Šæ›¿ãˆ
        if ROS_DISTRO == 'humble':
            self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', qos)
        else:
            self.cmd_vel_pub = self.create_publisher(TwistStamped, 'cmd_vel', qos)


        # ---- ã‚µãƒ–ã‚¹ã‚¯ãƒ©ã‚¤ãƒ–ï¼šã‚ªãƒ‰ãƒ¡ãƒˆãƒªã¨LiDAR ----
        self.odom_sub = self.create_subscription(Odometry, 'odom', self.odom_sub_callback, qos)
        self.scan_sub = self.create_subscription(LaserScan, 'scan', self.scan_sub_callback, qos_profile_sensor_data)

        # ---- ç’°å¢ƒåˆ¶å¾¡ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç¾¤ ----
        self.clients_callback_group = MutuallyExclusiveCallbackGroup()
        self.task_succeed_client = self.create_client(Goal, 'task_succeed', callback_group=self.clients_callback_group)
        self.task_failed_client = self.create_client(Goal, 'task_failed', callback_group=self.clients_callback_group)
        self.initialize_environment_client = self.create_client(Goal, 'initialize_env', callback_group=self.clients_callback_group)

        # ---- å¼·åŒ–å­¦ç¿’ãƒãƒ¼ãƒ‰ã¨ã®é€šä¿¡ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆã‚µãƒ¼ãƒï¼‰----
        self.rl_agent_interface_service = self.create_service(Dqn, 'rl_agent_interface', self.rl_agent_interface_callback)
        self.make_environment_service = self.create_service(Empty, 'make_environment', self.make_environment_callback)
        self.reset_environment_service = self.create_service(Dqn, 'reset_environment', self.reset_environment_callback)

    # ============================================================================
    # ç’°å¢ƒåˆæœŸåŒ–ï¼ãƒªã‚»ãƒƒãƒˆé–¢é€£
    # ============================================================================
    def make_environment_callback(self, request, response):
        """ç’°å¢ƒç”Ÿæˆï¼ˆã‚´ãƒ¼ãƒ«ä½ç½®åˆæœŸåŒ–ã‚’ãƒˆãƒªã‚¬ãƒ¼ï¼‰"""
        self.get_logger().info('Make environment called')
        while not self.initialize_environment_client.wait_for_service(timeout_sec=1.0):
            self.get_logger().warn('initialize_env service not available, waiting...')
        # initialize_envã‚’å‘¼ã³å‡ºã—ã¦ã‚´ãƒ¼ãƒ«ä½ç½®ã‚’å–å¾—
        future = self.initialize_environment_client.call_async(Goal.Request())
        rclpy.spin_until_future_complete(self, future)
        response_goal = future.result()
        if not response_goal or not response_goal.success:
            self.get_logger().error('initialize_env request failed')
        else:
            self.goal_pose_x = response_goal.pose_x
            self.goal_pose_y = response_goal.pose_y
            self.get_logger().info(f'Goal initialized at [{self.goal_pose_x:.2f}, {self.goal_pose_y:.2f}]')
        return response

    # ã“ã®é–¢æ•°ã¯æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒé–‹å§‹ã™ã‚‹ãŸã³ã«å‘¼ã³å‡ºã•ã‚Œã‚‹
    # ãƒ­ãƒœãƒƒãƒˆã®ç¾åœ¨ã®çŠ¶æ…‹ã‚’è¦³æ¸¬ã—ã€ãã‚Œã‚’åˆæœŸçŠ¶æ…‹ã¨ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«è¿”ã—ã¾ã™
    def reset_environment_callback(self, request, response):
        """ç’°å¢ƒãƒªã‚»ãƒƒãƒˆæ™‚ã®çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        state = self.calculate_state()
        self.init_goal_distance = state[0]
        self.prev_goal_distance = self.init_goal_distance
        response.state = state
        return response

    # ============================================================================
    # Gazeboç’°å¢ƒã¨ã®é€£æºã‚µãƒ¼ãƒ“ã‚¹ï¼ˆã‚´ãƒ¼ãƒ«æ›´æ–°ãªã©ï¼‰
    # ============================================================================
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚äº†ã—ãŸéš›ã«ã“ã®ãƒãƒ¼ãƒ‰è‡ªèº«ãŒå†…éƒ¨çš„ã«å‘¼ã³å‡ºã™ãƒ¡ã‚½ãƒƒãƒ‰

    # ã‚´ãƒ¼ãƒ«ã—ãŸå ´åˆ
    def call_task_succeed(self):
        """ã‚´ãƒ¼ãƒ«åˆ°é”æ™‚ã«å‘¼ã³å‡ºã—ï¼šæ¬¡ã®ã‚´ãƒ¼ãƒ«ã‚’ç”Ÿæˆ"""
        while not self.task_succeed_client.wait_for_service(timeout_sec=1.0):
            self.get_logger().warn('Waiting for task_succeed...')
        future = self.task_succeed_client.call_async(Goal.Request())
        rclpy.spin_until_future_complete(self, future)
        result = future.result()
        if result:
            self.goal_pose_x, self.goal_pose_y = result.pose_x, result.pose_y
            self.get_logger().info('Task succeed service finished')

    # éšœå®³ç‰©ã«è¡çªã¾ãŸã¯æ™‚é–“åˆ‡ã‚Œã«ãªã£ãŸå ´åˆ
    def call_task_failed(self):
        """è¡çªï¼ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã«å‘¼ã³å‡ºã—ï¼šã‚´ãƒ¼ãƒ«ã‚’å†é…ç½®"""
        while not self.task_failed_client.wait_for_service(timeout_sec=1.0):
            self.get_logger().warn('Waiting for task_failed...')
        future = self.task_failed_client.call_async(Goal.Request())
        rclpy.spin_until_future_complete(self, future)
        result = future.result()
        if result:
            self.goal_pose_x, self.goal_pose_y = result.pose_x, result.pose_y
            self.get_logger().info('Task failed service finished')

    # ============================================================================
    # ã‚»ãƒ³ã‚µã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆLiDAR + Odometryï¼‰
    # ============================================================================
    # ãƒ­ãƒœãƒƒãƒˆã®ã‚»ãƒ³ã‚µãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãŒé€ã‚‰ã‚Œã¦ãã‚‹ãŸã³ã«å®Ÿè¡Œã•ã‚Œã€ç”Ÿã®ãƒ‡ãƒ¼ã‚¿ã‚’å¼·åŒ–å­¦ç¿’ã§ä½¿ãˆã‚‹å½¢å¼ã®æƒ…å ±ã«å¤‰æ›
    def scan_sub_callback(self, scan):
        """LiDARã‚¹ã‚­ãƒ£ãƒ³ã‚’å—ä¿¡ã—ã€360Â°ã‚’24åˆ†å‰²ã—ã¦ä»£è¡¨è·é›¢ã‚’æŠ½å‡º"""
        # --- 1. LaserScan â†’ NumPyé…åˆ— ---
        raw_ranges = np.array(scan.ranges, dtype=np.float32)
        raw_ranges[np.isinf(raw_ranges)] = 3.5     # Infã‚’æœ€å¤§è·é›¢ã«ç½®æ›
        raw_ranges[np.isnan(raw_ranges)] = 1.5     # NaNã‚’0ã«ç½®æ›

        num_rays = len(raw_ranges)
        num_bins = 24                              # â˜… 24åˆ†å‰²
        step = num_rays // num_bins                # å„ã‚»ã‚¯ã‚¿ã®å¹…ï¼ˆä¾‹: 360/24=15ï¼‰

        # --- 2. å„15Â°ã‚»ã‚¯ã‚¿ã®ä»£è¡¨è·é›¢ï¼ˆæœ€å°å€¤ï¼‰ã‚’æŠ½å‡º ---
        sector_mins = []
        for i in range(num_bins):
            start = i * step
            end = (i + 1) * step
            sector_slice = raw_ranges[start:end]
            min_dist = float(np.min(sector_slice))  # éšœå®³ç‰©ã«æ•æ„Ÿãªã€Œæœ€å°å€¤ã€ã‚’æ¡ç”¨
            sector_mins.append(min_dist)

        # --- 3. çµæœã‚’ä¿å­˜ ---
        self.scan_ranges = sector_mins              # â† 24è¦ç´ å›ºå®š
        self.min_obstacle_distance = min(sector_mins)

    def odom_sub_callback(self, msg):
        """ã‚ªãƒ‰ãƒ¡ãƒˆãƒªæƒ…å ±ã‚’å—ä¿¡ã—ã€ãƒ­ãƒœãƒƒãƒˆå§¿å‹¢ã¨ã‚´ãƒ¼ãƒ«ç›¸å¯¾è§’ã‚’æ›´æ–°"""
        self.robot_pose_x = msg.pose.pose.position.x
        self.robot_pose_y = msg.pose.pose.position.y
        _, _, self.robot_pose_theta = self.euler_from_quaternion(msg.pose.pose.orientation)

        # ã‚´ãƒ¼ãƒ«ã¾ã§ã®è·é›¢ã¨è§’åº¦ã‚’è¨ˆç®—
        goal_distance = math.hypot(self.goal_pose_x - self.robot_pose_x, self.goal_pose_y - self.robot_pose_y)
        # 
        path_theta = math.atan2(self.goal_pose_y - self.robot_pose_y, self.goal_pose_x - self.robot_pose_x)
        goal_angle = path_theta - self.robot_pose_theta

        # [-Ï€, Ï€]ç¯„å›²ã«æ­£è¦åŒ–
        if goal_angle > math.pi:
            goal_angle -= 2 * math.pi
        elif goal_angle < -math.pi:
            goal_angle += 2 * math.pi

        self.goal_distance = goal_distance
        self.goal_angle = goal_angle

    # ============================================================================
    # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ãŠã‚ˆã³å ±é…¬è¨­è¨ˆ
    # ============================================================================
    def calculate_state(self):
        """ãƒ­ãƒœãƒƒãƒˆã®çŠ¶æ…‹ï¼ˆè·é›¢ãƒ»è§’åº¦ï¼‹360Â°â†’24åˆ†å‰²LiDARï¼‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        state = [float(self.goal_distance), float(self.goal_angle)]

        # â˜… å‰æ–¹ã®ã¿ã§ãªã360Â°åˆ†å‰²å€¤ã‚’ä½¿ç”¨
        for var in self.scan_ranges:
            state.append(float(var))

        # ---- ä»¥ä¸‹ã¯æ—¢å­˜ã®çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯ã‚’ãã®ã¾ã¾ç¶­æŒ ----
        if self.goal_distance < 0.20:
            self.get_logger().info('Goal Reached')
            self.succeed, self.done = True, True
            self.publish_stop()
            self.local_step = 0
            self.call_task_succeed()

        if self.min_obstacle_distance < 0.15:
            self.get_logger().info('Collision')
            self.fail, self.done = True, True
            self.publish_stop()
            self.local_step = 0
            self.call_task_failed()

        if self.local_step >= self.max_step:
            self.get_logger().info('Time Out')
            self.done = True          # â˜… ä¿®æ­£: done ãƒ•ãƒ©ã‚°ã®ã¿ True ã«ã™ã‚‹
            # self.fail = True        # â˜… å‰Šé™¤: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯ self.fail ã‚’ç«‹ã¦ãªã„
            self.publish_stop()
            self.local_step = 0
            self.call_task_failed()

        return state


    # def compute_directional_weights(self, relative_angles, max_weight=10.0):
    #     """å‰æ–¹æ–¹å‘ã«å¼·ãé‡ã¿ã‚’ã‹ã‘ã‚‹è§’åº¦ä¾å­˜é–¢æ•°"""
    #     power = 6
    #     raw = (numpy.cos(relative_angles)) ** power + 0.1
    #     scaled = raw * (max_weight / numpy.max(raw))
    #     return scaled / numpy.sum(scaled)

    # def compute_weighted_obstacle_reward(self):
    #     """LiDARè·é›¢ã«åŸºã¥ãéšœå®³ç‰©ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆå‰æ–¹é›†ä¸­å‹ï¼‰"""
    #     if not self.front_ranges or not self.front_angles:
    #         return 0.0
    #     front_ranges = numpy.array(self.front_ranges)
    #     front_angles = numpy.array(self.front_angles)
    #     mask = front_ranges <= 0.4
    #     if not numpy.any(mask):
    #         return 0.0
    #     front_ranges = front_ranges[mask]
    #     front_angles = front_angles[mask]
    #     rel_angles = numpy.unwrap(front_angles)
    #     rel_angles[rel_angles > numpy.pi] -= 2 * numpy.pi
    #     weights = self.compute_directional_weights(rel_angles)
    #     safe = numpy.clip(front_ranges - 0.20, 1e-2, 3.5)
    #     decay = numpy.exp(-2.0 * safe)
    #     weighted = numpy.dot(weights, decay)
    #     return - (0.8 + 2.5 * weighted)

    # # Lidaræœ€çŸ­è·é›¢ä¸€æœ¬ã ã‘ã‚’ãƒšãƒŠãƒ«ãƒ†ã‚£ã¨ã™ã‚‹
    # def compute_weighted_obstacle_reward(self):
        
    #     if not self.scan_ranges:
    #         return 0.0
        
    #     ranges_360 = numpy.array(self.scan_ranges)

    #     mask = ranges_360 <= 0.4
    #     if not numpy.any(mask):
    #         return 0.0
        
    #     close_ranges = ranges_360[mask]
    #     safe = numpy.clip(close_ranges - 0.20, 1e-2, 3.5) #
    #     decay = numpy.exp(-2.0 * safe) #

    #     weighted = numpy.sum(decay)
        
    #     return - (10 + 2.5 * weighted)

    # ã™ã¹ã¦ã®Lidarã«ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆä¸€å®šï¼‰
    # def compute_weighted_obstacle_reward(self):

    #     if not self.scan_ranges:
    #         return 0.0
        
    #     ranges = np.array(self.scan_ranges)

    #     # --- è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---
    #     threshold_dist = 0.30     
    #     fixed_penalty = 0.8       
    #     is_close = np.any(ranges < threshold_dist)

    #     if is_close:
    #         return -fixed_penalty
    #     else:
    #         return 0.0

    # ã™ã¹ã¦ã®Lidarã«ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæ¯”ä¾‹ï¼‰
    def compute_weighted_obstacle_reward(self):
 
        if not self.scan_ranges:
            return 0.0

        # NumPyé…åˆ—ã«å¤‰æ›
        ranges = np.array(self.scan_ranges)
 
        # ğŸ‘ˆ ä¿®æ­£: ã—ãã„å€¤ã‚’ 0.30m ã«å›ºå®š
        threshold_dist = 0.35  

        # ğŸ‘ˆ ä¿®æ­£: ä¿‚æ•°ã‚’ 0.081 ã«è¨­å®š (0.15mã§ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒ 3.00 ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´)
        penalty_scale  = 0.081
 
        mask = ranges < threshold_dist
        if not np.any(mask):
            return 0.0
 
        close_points = ranges[mask]
 
        # 0é™¤ç®—ã‚’é˜²ããŸã‚ã€éå¸¸ã«å°ã•ã„å€¤ (e.g., 1e-6) ã‚’è·é›¢ã«åŠ ç®—

        # 1. é€†æ•°ã®å·®åˆ†ã‚’è¨ˆç®—: (1 / è·é›¢) - (1 / ã—ãã„å€¤(0.30))
        diffs_inverse = (1.0 / (close_points + 1e-6)) - (1.0 / threshold_dist)

        # 2. ãã®å·®åˆ†ã‚’ä¸‰ä¹—ã™ã‚‹ (å¢—åŠ ç‡ã‚’æ€¥æ¿€ã«ã™ã‚‹)
        # â˜…â˜…â˜… power=3 ã‚’æ¡ç”¨ â˜…â˜…â˜…
        penalty_component = diffs_inverse ** 3

        total_penalty = np.sum(penalty_component) * penalty_scale
        return -total_penalty
 

    def calculate_reward(self):
        """å ±é…¬ã®è¨ˆç®—ã¨ãƒ­ã‚°å‡ºåŠ›ï¼ˆä¿®æ­£ï¼‰"""
        
        # 1. ã‚´ãƒ¼ãƒ«è·é›¢ã®è¨ˆç®—
        distance_to_goal = math.sqrt(
            (self.goal_pose_x - self.robot_pose_x) ** 2 +
            (self.goal_pose_y - self.robot_pose_y) ** 2
        )

        # åˆå›ã®ã¿åˆæœŸåŒ–
        if not hasattr(self, 'prev_distance'):
            self.prev_distance = distance_to_goal

        # 2. è·é›¢å ±é…¬ã®è¨ˆç®—
                # 2. è·é›¢å ±é…¬ã®è¨ˆç®—
        if self.local_step == 0:
            distance_diff = 0.0
        else:
            distance_diff = self.prev_distance - distance_to_goal
            # self.distance_reward = distance_diff * 300.0
            self.distance_reward = max(0.0, distance_diff) * 100.0

        # 3. è§’åº¦å ±é…¬ã®è¨ˆç®—
        self.yaw_reward = 0.0 * math.cos(self.goal_angle) # â˜… ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã«ä¿å­˜

        # 4. éšœå®³ç‰©ãƒšãƒŠãƒ«ãƒ†ã‚£ã®è¨ˆç®—
        self.obstacle_reward = self.compute_weighted_obstacle_reward() # â˜… ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã«ä¿å­˜

        # 5. çµ‚äº†æ™‚å ±é…¬ã®è¨ˆç®—
        steps = max(1, getattr(self, "_ep_steps", self.local_step))
        S = float(self.max_step)
        self.terminal_reward = 0.0 # â˜… ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã«ä¿å­˜
        if self.succeed:
            succ_scale = max(0.2, 1.0 - (steps - 1) / S) 
            self.terminal_reward = 600.0 * succ_scale
        elif self.fail:
            fail_scale = min(1.5, steps / S)
            self.terminal_reward = -500.0 * fail_scale  

        # 6. ç·å ±é…¬è¨ˆç®—
        reward = self.distance_reward + self.yaw_reward + self.obstacle_reward + self.terminal_reward

        # --- 7. å‰å›è·é›¢ã®æ›´æ–° ---
        self.prev_distance = distance_to_goal

        return reward
    
    # â˜… ã‚¨ãƒ©ãƒ¼è§£æ¶ˆã®ãŸã‚ã®è¿½åŠ ãƒ¡ã‚½ãƒƒãƒ‰ â˜…
    def get_reward_components(self):
        """è¨ˆç®—ã•ã‚ŒãŸå ±é…¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’è¾æ›¸ã§è¿”ã™ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
        return {
            'distance_reward': self.distance_reward,
            'yaw_reward': self.yaw_reward,
            'obstacle_reward': self.obstacle_reward,
            'terminal_reward': self.terminal_reward,
        }

    # ============================================================================
    # é€£ç¶šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆâ†’ç’°å¢ƒï¼‰
    # ============================================================================
    def rl_agent_interface_callback(self, request, response):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆPPOï¼‰ã‹ã‚‰é€£ç¶šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å—ã‘å–ã‚Šå®Ÿè¡Œ"""

        self._ep_steps = getattr(self, "_ep_steps", 0) + 1
        try:
            linear_v = float(np.clip(request.action[0], -0.06, 0.30))
            angular_v = float(np.clip(request.action[1], -1.5,  1.5))
        except Exception:
            linear_v = 0.0
            angular_v = 0.0

        # --- ãƒ‡ãƒƒãƒ‰ãƒãƒ³ãƒ‰ï¼ˆå¾®å°ãªèª¤å·®ã§å‹•ã‹ãªã„ã‚ˆã†ã«ï¼‰
        if abs(linear_v) < 0.005:
            linear_v = 0.0


        # --- Twist ç™ºè¡Œ ---
        if ROS_DISTRO == 'humble':
            msg = Twist()
            msg.linear.x = linear_v
            msg.angular.z = angular_v
        else:
            msg = TwistStamped()
            msg.twist.linear.x = linear_v
            msg.twist.angular.z = angular_v

        self.cmd_vel_pub.publish(msg)
        # self.restart_stop_timer()

        # --- çŠ¶æ…‹ã¨å ±é…¬ ---
        response.state = self.calculate_state()
        response.reward = self.calculate_reward() 
        response.done = self.done
        response.success = self.succeed

        self.local_step += 1
        reward_info = self.get_reward_components() # å ±é…¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å–å¾— (â˜… ã‚¨ãƒ©ãƒ¼è§£æ¶ˆ)

        # -------------------------------
        # ğŸ§­ 1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®è»Œè·¡ãƒ­ã‚°ã‚’è¨˜éŒ²
        # -------------------------------
        if not hasattr(self, 'trajectory_log') or self.local_step == 1:
            # self.trajectory_logã¯__init__ã§åˆæœŸåŒ–æ¸ˆã¿ã ãŒã€å¿µã®ãŸã‚episode_countã‚‚ãƒã‚§ãƒƒã‚¯
            self.episode_count = getattr(self, 'episode_count', 0)

        self.trajectory_log.append((
            self.episode_count + 1,        
            self.local_step,
            round(self.robot_pose_x, 3),
            round(self.robot_pose_y, 3),
            round(linear_v, 3),
            round(angular_v, 3),
            round(reward_info['distance_reward'], 3),
            round(reward_info['yaw_reward'], 3),
            round(reward_info['obstacle_reward'], 3),
            round(reward_info['terminal_reward'], 3),
            round(response.reward, 3)
        ))

        # -------------------------------
        # ğŸ¯ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ï¼šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        # -------------------------------
        if self.done:
            final_step = self.local_step

            self._ep_steps = 0
            self.episode_count += 1
            self.get_logger().info(f"ğŸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {self.episode_count} çµ‚äº†")
            self.get_logger().info("ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼")
            self.get_logger().info(f"çµ‚äº†ã‚¹ãƒ†ãƒƒãƒ—æ•°: {final_step}")
            self.get_logger().info("ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼")

            # 30ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«ä¿å­˜
            if self.episode_count % 5 == 0:
                # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å®šç¾©ã¨ä½œæˆ
                save_dir = os.path.join(
                    os.path.dirname(os.path.dirname(os.path.realpath(__file__))),
                    'saved_model'
                )
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)
                
                start_ep = self.episode_count - 4 
                end_ep = self.episode_count
                save_path = os.path.join(save_dir, f"trajectory_ep{start_ep}_to_{end_ep}.csv")
                
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ•´å½¢ã—ã¦ãƒ­ã‚°å‡ºåŠ›
                self.get_logger().info("\n\n" + "="*112)
                self.get_logger().info(f"ğŸ’¾ è»Œè·¡ãƒ­ã‚°ä¿å­˜ ({start_ep}ã€œ{end_ep})")
                self.get_logger().info("==================================================================================================================")
                
                # æ—¥æœ¬èªãƒ˜ãƒƒãƒ€ãƒ¼
                header = "| ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ | Step |   Xåº§æ¨™  |   Yåº§æ¨™  | ç·šå½¢é€Ÿåº¦ | è§’é€Ÿåº¦ | è·é›¢å ±é…¬ | è§’åº¦å ±é…¬ | éšœå®³ç‰©å ±é…¬ | çµ‚äº†æ™‚å ±é…¬ | ç·å ±é…¬ |"
                self.get_logger().info(header)
                # åŒºåˆ‡ã‚Šç·š
                self.get_logger().info("------------------------------------------------------------------------------------------------------------------")
                
                with open(save_path, "w") as f:
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ã¯è§£æã®ã—ã‚„ã™ã•ã®ãŸã‚è‹±èªå¤‰æ•°åã‚’ç¶­æŒ
                    f.write("episode,step,x,y,linear,angular,distance_reward,yaw_reward,obstacle_reward,terminal_reward,total_reward\n")
                    
                    # ä¿®æ­£ç®‡æ‰€: ã‚¿ãƒ—ãƒ«ã®è¦ç´ ã‚’æ—¥æœ¬èªã®å¤‰æ•°åã«å‰²ã‚Šå½“ã¦ã¦CSVã«æ›¸ãå‡ºã—
                    for (ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰, ã‚¹ãƒ†ãƒƒãƒ—, Xåº§æ¨™, Yåº§æ¨™, ç·šå½¢é€Ÿåº¦, è§’é€Ÿåº¦, è·é›¢å ±é…¬, è§’åº¦å ±é…¬, éšœå®³ç‰©å ±é…¬, çµ‚äº†æ™‚å ±é…¬, ç·å ±é…¬) in self.trajectory_log:
                        # CSVæ›¸ãå‡ºã— (å€¤ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æ›¸ãå‡ºã—)
                        f.write(f"{ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰},{ã‚¹ãƒ†ãƒƒãƒ—},{Xåº§æ¨™},{Yåº§æ¨™},{ç·šå½¢é€Ÿåº¦},{è§’é€Ÿåº¦},{è·é›¢å ±é…¬},{è§’åº¦å ±é…¬},{éšœå®³ç‰©å ±é…¬},{çµ‚äº†æ™‚å ±é…¬},{ç·å ±é…¬}\n")
                        
                        # ãƒ­ã‚°å‡ºåŠ›ã‚‚æ•´å½¢ã—ã¦è¡¨ç¤º
                        self.get_logger().info(
                            f"| {ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰:^10d} | {ã‚¹ãƒ†ãƒƒãƒ—:^4d} | {Xåº§æ¨™:^8.4f} | {Yåº§æ¨™:^8.4f} | {ç·šå½¢é€Ÿåº¦:^8.3f} | {è§’é€Ÿåº¦:^6.3f} | {è·é›¢å ±é…¬:^8.3f} | {è§’åº¦å ±é…¬:^8.3f} | {éšœå®³ç‰©å ±é…¬:^10.3f} | {çµ‚äº†æ™‚å ±é…¬:^10.3f} | {ç·å ±é…¬:^6.3f} |"
                        )
                
                self.get_logger().info("==================================================================================================================")
                self.get_logger().info(f"ğŸ’¾ 30ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†ã®è»Œè·¡ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
                self.trajectory_log = []  # ãƒ­ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ

            # çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
            self.publish_stop()

            time.sleep(0.3)
            self.done = False
            self.succeed = False
            self.fail = False
            self.local_step = 0

        self.last_cmd_lin = linear_v
        self.last_cmd_ang = angular_v


        return response

    def restart_stop_timer(self):
        """1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«çŸ­æ™‚é–“å¾Œãƒ­ãƒœãƒƒãƒˆã‚’åœæ­¢ã™ã‚‹ã‚¿ã‚¤ãƒãƒ¼ã‚’è¨­å®š"""
        if self.stop_cmd_vel_timer is not None:
            self.destroy_timer(self.stop_cmd_vel_timer)
        self.stop_cmd_vel_timer = self.create_timer(0.2, self.timer_callback)

    def timer_callback(self):
        """ã‚¿ã‚¤ãƒãƒ¼åˆ°é”æ™‚ã«åœæ­¢ã‚³ãƒãƒ³ãƒ‰ã‚’ç™ºè¡Œ"""
        self.publish_stop()
        self.destroy_timer(self.stop_cmd_vel_timer)

    def publish_stop(self):
        """ãƒ­ãƒœãƒƒãƒˆåœæ­¢ï¼ˆcmd_vel=0ï¼‰ã‚’ãƒ‘ãƒ–ãƒªãƒƒã‚·ãƒ¥"""
        if ROS_DISTRO == 'humble':
            self.cmd_vel_pub.publish(Twist())
        else:
            self.cmd_vel_pub.publish(TwistStamped())

    # ============================================================================
    # Utilityé–¢æ•°
    # ============================================================================
    def euler_from_quaternion(self, quat):
        """å››å…ƒæ•° â†’ ã‚ªã‚¤ãƒ©ãƒ¼è§’ï¼ˆroll, pitch, yawï¼‰å¤‰æ›"""
        x, y, z, w = quat.x, quat.y, quat.z, quat.w
        sinr_cosp = 2 * (w * x + y * z)
        cosr_cosp = 1 - 2 * (x * x + y * y)
        roll = numpy.arctan2(sinr_cosp, cosr_cosp)
        sinp = 2 * (w * y - z * x)
        pitch = numpy.arcsin(sinp)
        siny_cosp = 2 * (w * z + x * y)
        cosy_cosp = 1 - 2 * (y * y + z * z)
        yaw = numpy.arctan2(siny_cosp, cosy_cosp)
        return roll, pitch, yaw


# ============================================================================
# ROSãƒãƒ¼ãƒ‰ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
# ============================================================================
def main(args=None):
    rclpy.init(args=args)
    env = RLEnvironment()
    try:
        # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ï¼šROSã‚¹ãƒ”ãƒ³ã§ã‚µãƒ¼ãƒ“ã‚¹ã¨è³¼èª­ã‚’å‡¦ç†
        while rclpy.ok():
            rclpy.spin_once(env, timeout_sec=0.1)
    except KeyboardInterrupt:
        pass
    finally:
        env.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()